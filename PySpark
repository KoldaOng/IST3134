sudo su - hadoop

cd IST3134/

wget https://github.com/KoldaOng/IST3134/raw/main/asg-dataset.zip

unzip asg-dataset.zip

cd /home/IST3134/asg-dataset/

nano analysis.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, explode, lower, regexp_replace, count, lit

# Start Spark
spark = SparkSession.builder \
    .appName("Review Keyword Analysis") \
    .getOrCreate()

# Load CSV (adjust path if needed)
df = spark.read.csv("cleaned_all_review.csv", header=True, inferSchema=True)

# Basic text cleaning
df = df.withColumn("pros_clean", lower(regexp_replace(col("pros"), "[^a-zA-Z\s]", ""))) \
       .withColumn("cons_clean", lower(regexp_replace(col("cons"), "[^a-zA-Z\s]", "")))

# Tokenize into words and tag sentiment
pros_words = df.withColumn("word", explode(split(col("pros_clean"), "\s+"))) \
               .withColumn("sentiment", lit("positive"))
cons_words = df.withColumn("word", explode(split(col("cons_clean"), "\s+"))) \
               .withColumn("sentiment", lit("negative"))

# Remove empty strings
pros_words = pros_words.filter(col("word") != "")
cons_words = cons_words.filter(col("word") != "")

# Stopwords
stopwords = [
    "the","and","for","are","with","that","this","was","were","but","not","you","your",
    "their","they","our","from","have","has","had","its","it","on","in","to","of",
    "as","is","be","by","an","or","at","we","us","i","me","my","so","if","can","could",
    "would","should","very","more","most","also","too","than","then","when","while","into",
    "over","out","up","down","across","about","after","before","again","just","there","any",
    "yes","no","a","which","will","he","she","them","been","all","get","do"
]

# Remove stopwords
pros_words = pros_words.filter(~col("word").isin(stopwords))
cons_words = cons_words.filter(~col("word").isin(stopwords))

# Combine pros & cons
combined = pros_words.union(cons_words)

# Count frequency per sentiment
freq_table = combined.groupBy("sentiment", "word") \
                     .count() \
                     .orderBy(col("sentiment"), col("count").desc())

# Show top 20 for each sentiment
from pyspark.sql.window import Window
import pyspark.sql.functions as F

w = Window.partitionBy("sentiment").orderBy(col("count").desc())
top10_each = freq_table.withColumn("rank", F.row_number().over(w)) \
                       .filter(col("rank") <= 10) \
                       .orderBy("sentiment", "rank")

top10_each.show(20, truncate=False)  # 10 positive + 10 negative

spark-submit analysis.py
