# Update all available packages
sudo apt update

# Install AWS Command Line Interface (CLI)
sudo apt install awscli

# Go into hadoop server
sudo su - hadoop

# Start all Hadoop services
start-all.sh

# Load the dataset from S3 bucket 
aws s3 cp s3://bdagroupbucket112/cleaned_all_review.csv /home/hadoop/cleaned_all_review.csv

# Put the file into Hadoop
hdfs dfs -put /home/hadoop/cleaned_all_review.csv /user/hadoop/

# Create mapper python script
nano mapper.py

------------------------------Start of mapper script---------------------------------------------------
#!/usr/bin/env python3
import sys, csv, os, re
from datetime import datetime

RATING_COL = os.getenv("RATING_COL", "rating")
PROS_COL   = os.getenv("PROS_COL",   "pros")
CONS_COL   = os.getenv("CONS_COL",   "cons")
DATE_COL   = os.getenv("DATE_COL",   "review_date")

POS_MIN = float(os.getenv("POS_MIN", "4"))
NEG_MAX = float(os.getenv("NEG_MAX", "2"))
DATE_GRAIN = os.getenv("DATE_GRAIN", "year")  # year|month|day|all

MIN_LEN = int(os.getenv("MIN_LEN", "3"))
STOPWORDS = {
    "the","and","for","are","with","that","this","was","were","but","not","you","your",
    "their","they","our","from","have","has","had","its","it","on","in","to","of",
    "as","is","be","by","an","or","at","we","us","i","me","my","so","if","can","could",
    "would","should","very","more","most","also","too","than","then","when","while","into",
    "over","out","up","down","across","about","after","before","again","just","there","any",
    "yes","no","a","which","will","he","she","them","been", "all", "get", "do"
}
TOKEN_RE = re.compile(r"[A-Za-z][A-Za-z\-']+")

def iter_words(text):
    if not text: return
    for m in TOKEN_RE.finditer(text):
        w = m.group().lower().strip("'-")
        if len(w) >= MIN_LEN and w not in STOPWORDS:
            yield w

def period_from_date(s):
    if not s or DATE_GRAIN == "all": return "all"
    for fmt in ("%Y-%m-%d","%d-%m-%Y","%Y/%m/%d","%m/%d/%Y","%Y-%m","%Y/%m","%Y"):
        try:
            from datetime import datetime
            dt = datetime.strptime(s[:len(fmt)], fmt)
            break
        except: dt = None
    if dt is None:
        import re
        m = re.search(r"(\d{4})", s)
        return m.group(1) if m else "all"
    if DATE_GRAIN == "year":  return f"{dt.year:04d}"
    if DATE_GRAIN == "month": return f"{dt.year:04d}-{dt.month:02d}"
    if DATE_GRAIN == "day":   return f"{dt.year:04d}-{dt.month:02d}-{dt.day:02d}"
    return "all"

def emit(key_parts, value):
    sys.stdout.write("\t".join(key_parts) + "\t" + str(value) + "\n")

def main():
    reader = csv.DictReader(sys.stdin)
    for row in reader:
        r = (row.get(RATING_COL) or "").strip()
        if not r: continue
        try: rating = float(r)
        except: continue

        if rating >= POS_MIN:
            bucket, text = "positive", row.get(PROS_COL) or ""
        elif rating <= NEG_MAX:
            bucket, text = "negative", row.get(CONS_COL) or ""
        else:
            continue

        period = period_from_date(row.get(DATE_COL, "") or "")
        for tok in iter_words(text):
            # Key: sentiment | period | token
            emit([bucket, period, tok], 1)

if __name__ == "__main__":
    main()

------------------------------End of mapper script---------------------------------------------------

# Create reducer python script
nano reducer.py

------------------------------Start of reducer script---------------------------------------------------
#!/usr/bin/env python3
import sys, os, heapq

# How many words per (sentiment, period)
TOP_N = int(os.getenv("TOP_N", "10"))

def flush_group(group_key, word_counts):
    """Emit top-N for the finished (sentiment, period) group."""
    if group_key is None:
        return
    sentiment, period = group_key

    # Max-heap by negative count, then token asc for tie-break
    heap = [(-cnt, tok) for tok, cnt in word_counts.items()]
    heapq.heapify(heap)

    n = 0
    while heap and n < TOP_N:
        negcnt, tok = heapq.heappop(heap)
        cnt = -negcnt
        # Output: sentiment \t period \t word \t count
        sys.stdout.write(f"{sentiment}\t{period}\t{tok}\t{cnt}\n")
        n += 1

def main():
    current_group = None        # (sentiment, period)
    word_counts = {}            # token -> count

    for line in sys.stdin:
        parts = line.rstrip("\n").split("\t")
        # Expect mapper to output: sentiment, period, token, count
        if len(parts) == 4:
            sentiment, period, token, count_str = parts
        elif len(parts) == 3:
            # If mapper didn’t include a count, assume 1
            sentiment, period, token = parts
            count_str = "1"
        else:
            continue

        try:
            count = int(count_str)
        except:
            continue

        group_key = (sentiment, period)

        # When the sorted key changes, flush the previous group
        if group_key != current_group:
            flush_group(current_group, word_counts)
            word_counts.clear()
            current_group = group_key

        # Aggregate counts within the group
        word_counts[token] = word_counts.get(token, 0) + count

    # Flush last group
    flush_group(current_group, word_counts)

if __name__ == "__main__":
    main()
------------------------------End of reducer script---------------------------------------------------

# Run MapReduce locally on 'cleaned_all_review.csv' and display top 20 results (10 positive, 10 negative)
cat cleaned_all_review.csv | python3 mapper.py | sort | TOP N=10 python3 reducer.py | head -n 20
